text_lm:
  llama1_7b:
    pretrained: "meta-llama/Llama-2-7b-hf"
    arch: "llama"
    torch_dtype: "float16"
    layer: 32

  llama3_8b:
    pretrained: "meta-llama/Meta-Llama-3-8B-Instruct"
    arch: "auto"
    torch_dtype: "float16"
    layer: 32

  llama1_13b:
    pretrained: "/llama-13b-hf"
    arch: "llama"
    torch_dtype: "float16"
    layer: 40

  llama1_30b:
    pretrained: "/llama-30b-hf"
    arch: "llama"
    torch_dtype: "float16"
    layer: 60

  llama1_65b:
    pretrained: "/llama-65b-hf"
    arch: "llama"
    torch_dtype: "float16"
    layer: 80

v2t:
  llava1.5_7b:
    pretrained: "/mnt/data2/yangmrl/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/b234b804b114d9e37bb655e11cbbb5f5e971b7a9"
    arch: "llava"
    torch_dtype: "float16"
    layer: 32
  
  instructblip_vicuna_7b:
    pretrained: "/mnt/data2/yangmrl/instructblip_vicuna_7b"
    arch: "instruct"
    torch_dtype: "float16"
    layer: 32
  
  llavanext_8b:
    pretrained: "/mnt/data2/zjx/project/checkpoints/llama3-llava-next-8b-hf"
    arch: "llavanext"
    torch_dtype: "float16"
    layer: 32
  
  minigpt4_7b:
    pretrained: "/mnt/data1/yangmrl/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/b234b804b114d9e37bb655e11cbbb5f5e971b7a9"
    arch: "llava"
    torch_dtype: "float16"
    layer: 32
    
classifier:
  roberta-base:
    pretrained: "/mnt/data1/yangmrl/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b"
    arch: "RobertaForMaskedLM"
    torch_dtype: "float16"

# TODO
  bert-base:
    pretrained: "/mnt/data1/zjx/project/checkpoints/bert-base-uncased"
    arch: "BertForMaskedLM"
    torch_dtype: "float16"

  roberta-large:
    pretrained: "/mnt/data1/zjx/project/checkpoints/roberta-large"
    arch: "RobertaForMaskedLM"
    torch_dtype: "float16"




# QWenLMHeadModel(
#   (transformer): QWenModel(
#     (wte): Embedding(151936, 4096)
#     (drop): Dropout(p=0.0, inplace=False)
#     (rotary_emb): RotaryEmbedding()
#     (h): ModuleList(
#       (0-31): 32 x QWenBlock(
#         (ln_1): RMSNorm()
#         (attn): QWenAttention(
#           (c_attn): Linear(in_features=4096, out_features=12288, bias=True)
#           (c_proj): Linear(in_features=4096, out_features=4096, bias=False)
#           (attn_dropout): Dropout(p=0.0, inplace=False)
#         )
#         (ln_2): RMSNorm()
#         (mlp): QWenMLP(
#           (w1): Linear(in_features=4096, out_features=11008, bias=False)
#           (w2): Linear(in_features=4096, out_features=11008, bias=False)
#           (c_proj): Linear(in_features=11008, out_features=4096, bias=False)
#         )
#       )
#     )
#     (ln_f): RMSNorm()
#     (visual): VisionTransformer(
#       (conv1): Conv2d(3, 1664, kernel_size=(14, 14), stride=(14, 14), bias=False)
#       (ln_pre): LayerNorm((1664,), eps=1e-06, elementwise_affine=True)
#       (transformer): TransformerBlock(
#         (resblocks): ModuleList(
#           (0-47): 48 x VisualAttentionBlock(
#             (ln_1): LayerNorm((1664,), eps=1e-06, elementwise_affine=True)
#             (ln_2): LayerNorm((1664,), eps=1e-06, elementwise_affine=True)
#             (attn): VisualAttention(
#               (in_proj): Linear(in_features=1664, out_features=4992, bias=True)
#               (out_proj): Linear(in_features=1664, out_features=1664, bias=True)
#             )
#             (mlp): Sequential(
#               (c_fc): Linear(in_features=1664, out_features=8192, bias=True)
#               (gelu): GELU(approximate='none')
#               (c_proj): Linear(in_features=8192, out_features=1664, bias=True)
#             )
#           )
#         )
#       )
#       (attn_pool): Resampler(
#         (kv_proj): Linear(in_features=1664, out_features=4096, bias=False)
#         (attn): MultiheadAttention(
#           (out_proj): NonDynamicallyQuantizableLinear(in_features=4096, out_features=4096, bias=True)
#         )
#         (ln_q): LayerNorm((4096,), eps=1e-06, elementwise_affine=True)
#         (ln_kv): LayerNorm((4096,), eps=1e-06, elementwise_affine=True)
#       )
#       (ln_post): LayerNorm((4096,), eps=1e-06, elementwise_affine=True)
#     )
#   )
#   (lm_head): Linear(in_features=4096, out_features=151936, bias=False)
# )
    